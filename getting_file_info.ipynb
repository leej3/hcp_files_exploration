{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('max_colwidth',500)\n",
    "%matplotlib inline\n",
    "\n",
    "# dir = {'900':'/data/HCP/HCP_900/s3/hcp/**/*','1200': '/data/HCP/HCP_1200/**/*'}\n",
    "dirs = list(Path('/data/HCP/HCP_900/s3/hcp').iterdir())\n",
    "# cols = ['perms','links','user','group','size','year','time','dir']\n",
    "cols = ['size','date','dir']\n",
    "pklz_dir = Path('output_pklz')\n",
    "if not pklz_dir.exists():\n",
    "    pklz_dir.mkdir()\n",
    "summary_pklz = pklz_dir.joinpath('summary.pklz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subdir(d_path):\n",
    "    return d_path.name\n",
    "def get_output_path(d_path):\n",
    "    return Path('output_pklz').joinpath('files_characterization_' + d_path.parent.name + '_' + get_subdir(d_path) + '.pklz')\n",
    "def write_tsv_with_atimes_and_size(d_path):\n",
    "        #     -lu gives access time\n",
    "    #     -d1 gives just the file/dir instead of the contents\n",
    "    #     the glob pattern provides all the files/dirs\n",
    "    #     the awk command turns it into tab separated output\n",
    "    # need to have globstar set to on in bash: shopt -s globstar\n",
    "    output_file = get_output_path(d_path)\n",
    "    print(d_path.as_posix() + '/**/*')\n",
    "    ! shopt -s globstar;ls -lu -d1  --time-style long-iso {d_path.as_posix() + '/**/*'}| awk -v OFS=\"\\t\" '$1=$1'|cut -f5,6,8 > {output_file.with_suffix('.tsv')}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write pickle for every subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir = {'test': '/data/HCP/HCP_1200/download_swarm/**/*'}\n",
    "for d_path in dirs:\n",
    "    output_file = get_output_path(d_path)\n",
    "    if not output_file.exists():\n",
    "        write_tsv_with_atimes_and_size(d_path)\n",
    "        df = pd.read_csv(output_file.with_suffix('.tsv'),sep = '\\t',names=cols, dtype = {'size':np.int32,'date':str,'dir' : str })\n",
    "        df.to_pickle(output_file)\n",
    "        output_file.with_suffix('.tsv').unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load 50 random subjects and assess the access times in their file trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_least_common_value(series):\n",
    "    return series[series.apply(len).idxmin()]\n",
    "def get_dir_level_summary(df,level = 7):\n",
    "    df_grouped = (\n",
    "        df.loc[pd.notnull(df[level+ 1]) ,:].\n",
    "        groupby(list(range(level + 1)))\n",
    "    )\n",
    "    df = (\n",
    "        df_grouped.\n",
    "        aggregate({'date':max,'size': sum, 'file':len,'parent_dir' : lambda x :get_least_common_value(x)}).\n",
    "        assign(total_size_gb = lambda df: round(df['size'] /1000000000,3)).\n",
    "        rename(columns = {'date':'most_recent_access',\n",
    "                     'file' : 'num_files'}).\n",
    "#         reset_index(drop = True).\n",
    "        assign(tree_depth = level)\n",
    "        \n",
    "    )\n",
    "    return df\n",
    "    \n",
    "# test = pd.concat([df_sub.head(100), df_sub.head(100).file.str.split('/',expand = True)], axis = 1)\n",
    "# get_dir_level_summary(test, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create merged dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del df_full\n",
    "except:\n",
    "    pass\n",
    "\n",
    "    \n",
    "for d_path in np.random.choice( dirs, 50):\n",
    "    output_file = get_output_path(d_path)\n",
    "    df_sub = pd.read_pickle(output_file)\n",
    "    df_sub = df_sub.rename(columns = {'dir' : 'file'})\n",
    "    df_sub['subject'] = d_path.name\n",
    "    df_sub['is_file'] = df_sub.file.apply(lambda x: Path(x).is_file())\n",
    "    df_sub['parent_dir'] = df_sub.file.apply(lambda x:'/'.join(x.split('/')[:-1]))\n",
    "\n",
    "    if 'df_full' in locals():\n",
    "        next_ind = df_full.index.max() + 1\n",
    "        df_full  = pd.concat([df_full,df_sub],axis = 0)\n",
    "    else:\n",
    "        df_full = df_sub.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split = pd.concat([df_full, df_full.file.str.split('/',expand = True)], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate summary dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.core.debugger import Pdb; ipdb=Pdb()\n",
    "# ipdb.runcall(get_dir_level_summary, df_split, 8)\n",
    "df_summary = get_dir_level_summary(df_split, 10)\n",
    "df_summary.to_pickle(summary_pklz)\n",
    "df_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary.info()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
