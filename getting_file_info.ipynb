{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook:\n",
    "1. With access to the a directory containing the hcp files, creates a pickle for each subject in the output_pklz directory\n",
    "2. Creates a dictionary calls dfs. \n",
    "    + dfs contains a dataframe summarizing the files at two levels of depth in the hcp filetree.\n",
    "    + dfs is written to a small pickled file called 100_subjects.pklz \n",
    "3. Shows a quick summary of the data. Main conclusion is that the bulk of files, which are those contained in the results directories of 'MNINonLinear', were recently accessed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import seaborn as sns\n",
    "import pickle\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('max_colwidth',500)\n",
    "# %matplotlib inline\n",
    "\n",
    "# dir = {'900':'/data/HCP/HCP_900/s3/hcp/**/*','1200': '/data/HCP/HCP_1200/**/*'}\n",
    "dirs = list(Path('/data/HCP/HCP_900/s3/hcp').iterdir())\n",
    "# cols = ['perms','links','user','group','size','year','time','dir']\n",
    "cols = ['size','date','dir']\n",
    "pklz_dir = Path('output_pklz')\n",
    "if not pklz_dir.exists():\n",
    "    # alternatively if subject_pklz.tar.gz exists this could be unpickled\n",
    "    pklz_dir.mkdir()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subdir(d_path):\n",
    "    return d_path.name\n",
    "def get_output_path(d_path):\n",
    "    return Path('output_pklz').joinpath('files_characterization_' + d_path.parent.name + '_' + get_subdir(d_path) + '.pklz')\n",
    "def write_tsv_with_atimes_and_size(d_path):\n",
    "        #     -lu gives access time\n",
    "    #     -d1 gives just the file/dir instead of the contents\n",
    "    #     the glob pattern provides all the files/dirs\n",
    "    #     the awk command turns it into tab separated output\n",
    "    # need to have globstar set to on in bash: shopt -s globstar\n",
    "    output_file = get_output_path(d_path)\n",
    "    print(d_path.as_posix() + '/**/*')\n",
    "    ! shopt -s globstar;ls -lu -d1  --time-style long-iso {d_path.as_posix() + '/**/*'}| awk -v OFS=\"\\t\" '$1=$1'|cut -f5,6,8 > {output_file.with_suffix('.tsv')}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write pickle for every subject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell was used to generate a tsv file for each subject:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# dir = {'test': '/data/HCP/HCP_1200/download_swarm/**/*'}\n",
    "for d_path in dirs:\n",
    "    output_file = get_output_path(d_path)\n",
    "    if not output_file.exists():\n",
    "        write_tsv_with_atimes_and_size(d_path)\n",
    "        df = pd.read_csv(output_file.with_suffix('.tsv'),sep = '\\t',names=cols, dtype = {'size':np.int32,'date':str,'dir' : str })\n",
    "        df.to_pickle(output_file)\n",
    "        output_file.with_suffix('.tsv').unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output pickles were subsequently tarred:\n",
    "tar -czvf subject_pklz.tar.gz output_pklz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load 100 random subjects and assess the access times in their file trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_least_common_value(series):\n",
    "    return series[series.apply(len).idxmin()]\n",
    "def get_dir_level_summary(df,level = 7):\n",
    "    df_grouped = (\n",
    "        df.loc[pd.notnull(df[level+ 1]) ,:].\n",
    "        groupby(list(range(level + 1)))\n",
    "    )\n",
    "    df = (\n",
    "        df_grouped.\n",
    "        aggregate({'date':max,'size': sum, 'file':len,'parent_dir' : lambda x :get_least_common_value(x)}).\n",
    "        assign(total_size_gb = lambda df: round(df['size'] /1000000000,3)).\n",
    "        rename(columns = {'date':'most_recent_access',\n",
    "                     'file' : 'num_files'}).\n",
    "#         reset_index(drop = True).\n",
    "        assign(tree_depth = level)\n",
    "        \n",
    "    )\n",
    "    return df\n",
    "\n",
    "# from IPython.core.debugger import Pdb; ipdb=Pdb()\n",
    "# ipdb.runcall(get_dir_level_summary, df_split, 8)\n",
    "\n",
    "# test = pd.concat([df_sub.head(100), df_sub.head(100).file.str.split('/',expand = True)], axis = 1)\n",
    "# get_dir_level_summary(test, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarise_subject_info(subject_summary,num_subs=3,levels=[7]):\n",
    "    dfs = {}\n",
    "    dfs_full = {}\n",
    "    if not subject_summary.exists():\n",
    "        for d_path in np.random.choice( dirs, num_subs):\n",
    "            output_file = get_output_path(d_path)\n",
    "            df_sub = pd.read_pickle(output_file)\n",
    "            df_sub = df_sub.rename(columns = {'dir' : 'file'})\n",
    "            df_sub['subject'] = d_path.name\n",
    "            df_sub['is_file'] = df_sub.file.apply(lambda x: Path(x).is_file())\n",
    "            df_sub['parent_dir'] = df_sub.file.apply(lambda x:'/'.join(x.split('/')[:-1]))\n",
    "#             Create columns representing depth into the file tree to group across them:\n",
    "            df_sub = pd.concat([df_sub, df_sub.file.str.split('/',expand = True)], axis = 1)\n",
    "            \n",
    "            for lev in levels:\n",
    "                dfs[lev] = get_dir_level_summary(df_sub, lev)\n",
    "\n",
    "                if lev in dfs_full.keys():\n",
    "                    dfs_full[lev]  = pd.concat([dfs_full[lev],dfs[lev]],axis = 0)\n",
    "                else:\n",
    "                    dfs_full[lev] = dfs[lev].copy()\n",
    "\n",
    "        pickle.dump(dfs_full, open(subject_summary, \"wb\"))\n",
    "    else: dfs_full = pickle.load(open(subject_summary, \"rb\"))\n",
    "    return dfs_full\n",
    "\n",
    "# directory of output pickles required:\n",
    "# tar xvf subject_pklz.tar.gz output_pklz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create merged dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below creates a dictionary of dataframes. Each dataframe contains a summary of files at the depth into the tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = summarise_subject_info(Path('100_subjects.pklz'), num_subs= 100, levels=[7,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[7].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[9].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Size of 'MNINonLinear' results directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the disk space of the hpc dataset is used up by the results directory in the 'MNINonLinear' directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all 100 subjects all the files have a size (in GB) of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[9].loc['','data','HCP','HCP_900','s3','hcp',:,:]['total_size_gb'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all 100 subjects MNINonLinear results files have a size (in GB) of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[9].loc['','data','HCP','HCP_900','s3','hcp',:,'MNINonLinear']['total_size_gb'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most recent access times in the MNINonLinear results directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "9 levels deep in the tree all directories in MNINonLinear/*/Results have been accessed recently. This is the bulk of the data in the HCP dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[9].loc['','data','HCP','HCP_900','s3','hcp',:,'MNINonLinear'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[9].loc['','data','HCP','HCP_900','s3','hcp',:,'MNINonLinear']['most_recent_access'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE CONDA ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!   conda env export > hcp_characterization.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
